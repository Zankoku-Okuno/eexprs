<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Eric Demko" />
  <title>Report on E-Exprs</title>
  <style>
    p {
      text-align: justify;
    }
    .Rationale::before {
      content: "Rationale";
      font-size: 110%;
      font-style: italic;
    }
    .Rationale {
      margin-left: 1em;
      margin-right: 1em;
      font-size: 80%;
    }
    .Rationale > p:first-child {
      margin-top: 0.4em;
    }
    .Warning::before {
      content: "Warning";
      font-size: 110%;
      font-weight: bold;
    }
    .Warning {
      margin-left: 1em;
      margin-right: 1em;
      font-size: 90%;
    }
    .Warning > p:first-child {
      margin-top: 0.4em;
    }
  </style>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Report on E-Exprs</h1>
<p class="author">Eric Demko</p>
</header>
<p>This report describes a grammar and parser for a data language with optimized ergonomics for encoding a wide range of programming languages. They key technical idea—which has been used to great effect since the dawn of high-level languages<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>—is to define the concrete syntax of one language as a subset of the concrete syntax of a previously-implemented data language. Thus, when implementing a new concrete syntax, one can re-use all of the work of converting a byte stream into a algebraic data type (ADT), and need only perform functional pattern matching to inject it into a type representing your abstract syntax. The main contribution is a selection of syntax that is familiar and therefore intuitive for programmers to read and write. The rest of this report will define eexprs, explicate the implementation technique, and motivate/rationalize my design choices.</p>
<h2 id="motivation">Motivation</h2>
<p>I’ve lost count of how many toy language implementations I’ve given up on after having put not-insignificant time into the parser; I’m <em>incredibly bored of writing parsers</em>. So, I resolved to stop writing them. As Lisp programmers are already aware, code is data: one obvious technique would be to encode the abstract syntax in an existing data format. While JSON parsers are readily available, actually reading or writing a term is physically painful. Other common data languages like XML and YAML suffer a similar fate. I think anyone would much prefer s-exprs or e-exprs.</p>
<dl>
<dt>JSON</dt>
<dd><pre><code>{ &quot;_type&quot;: &quot;apply&quot;, &quot;function&quot;: &quot;push&quot;
, &quot;args&quot;: [ &quot;x&quot;, {&quot;_type&quot;: &quot;list&quot;, &quot;elems&quot;:
  [ {&quot;_type&quot;: &quot;apply&quot;, &quot;function&quot;: &quot;f&quot;, args: [1]}
  , {&quot;_type&quot;: &quot;apply&quot;, &quot;function&quot;: &quot;g&quot;, args: [2]}
  ]} ]
}</code></pre>
</dd>
<dt>E-exprs</dt>
<dd><code>push x [f 1, g 2]</code>
</dd>
<dt>S-exprs</dt>
<dd><code>(push x (list (f 1) (g 2)))</code>
</dd>
<dt>YAML</dt>
<dd><pre><code>!apply
function: push
args:
  - x
  - - !apply {function: f, args: [1]}
    - !apply {function: g, args: [2]}</code></pre>
</dd>
<dt>XML</dt>
<dd><pre><code>&lt;apply&gt;
  &lt;function&gt;
    &lt;var&gt;push&lt;/var&gt;
  &lt;/function&gt;
  &lt;args&gt;
    &lt;var&gt;x&lt;/var&gt;
    &lt;list&gt;
      &lt;apply&gt;
        &lt;function&gt;f&lt;/function&gt;
        &lt;args&gt;&lt;int&gt;1&lt;/int&gt;&lt;/args&gt;
      &lt;/apply&gt;
      &lt;apply&gt;
        &lt;function&gt;g&lt;/function&gt;
        &lt;args&gt;&lt;int&gt;2&lt;/int&gt;&lt;/args&gt;
      &lt;/apply&gt;
    &lt;/list&gt;
  &lt;/args&gt;
&lt;/apply&gt;</code></pre>
</dd>
</dl>
<p>The most widely-known data languages are optimized for machine-machine communication, with only some consideration for humans to peek at it occasionally. However, writing source code is all about human-human communication, with only some consideration for the ability of a machine to read it. Both s-exprs and e-exprs are much better for this task.</p>
<h3 id="recognizer-technique-with-sexprs">Recognizer Technique with Sexprs</h3>
<p>S-exprs are the main inspiration for s-exprs. The two languages share a number of common features, but sexprs are a much smaller language. Therefore, I’ll go over sexprs first to illustrate the technique succinctly before demonstrating eexprs. Though I have tried to distill the essence of this particular advantage of sexprs, if you are already familiar with this technique, it still might seem like flogging a dead horse.</p>
<p>The key idea is to separate parsing for a target language into two stages:</p>
<ol type="1">
<li>Parse a byte stream into the abstract syntax (i.e. data type) of s-exprs.</li>
<li>Recursively pattern match against the s-expr value to construct a term in the target language’s abstract syntax.</li>
</ol>
<p>In the case of sexprs, we have a particularly small abstract syntax:</p>
<pre><code>data SExpr
  = Atom Text
  | Combination [SExpr]</code></pre>
<p>Although one might have an idea about what the values of this type mean (e.g. that combinations are function calls), sexprs need not be a carrier for only Lisp dialects. One could just as easily use sexprs to represent the abstract syntax of (say) Prolog or Algol.</p>
<p>To use s-exprs as part of a lambda calculus implementation, we need only define our abstract syntax,</p>
<pre><code>data Lam
  = Var Text
  | Lam [Text] [Lam]
  | App Lam [Lam]</code></pre>
<p>and then define a recognizer,</p>
<pre><code>recognize :: Sexpr -&gt; Maybe Lam
recognize (Atom name)
  = Just (Var name)
recognize (Combination (Atom &quot;lambda&quot; : params : body)
  = Lam &lt;$&gt; map recognizeParams params &lt;*&gt; map recognize body
  where
  recognizeParams (Atom name) = Just name
  recognizeParams _ = Nothing
recognize (Combination (func : args)
  = App &lt;$&gt; recognize func &lt;*&gt; map recognize args
recognize _ = Nothing</code></pre>
<p>and with only this handful of code, we have completed an entire parser for the lambda calculus.</p>
<p>Admittedly, this recognizer does not report informative errors. Later, we will see a “grammar combinator” library for e-exprs that tracks context and can therefore support much richer error diagnostics.</p>
<p>The advantages of the technique are</p>
<ul>
<li>A high-quality s-expr parser and grammar combinator library need only be implemented once, and then it can be shared between multitudes of new languages.</li>
<li>Most of the tedious and tricky parts of parsing are in the s-expr parser; the code that needs to be written for new languages is small and simple.</li>
</ul>
<p>The question now is, if s-exprs are so great, why implement e-exprs? My view is that s-exprs, while they are far more ergonomic than JSON, are <em>not</em> ergonomic compared to common programming languages. A claim like that is certain to spark vocal argument, but to engage in it here would derail the main thrust of this paper. Instead, let’s jump straight into describing e-exprs and delay any rant-like objects until <a href="#suboptimality-of-s-exprs">later in the report</a>. Hopefully, the advantages of e-exprs will become clear as we examine them.</p>
<h2 id="a-whirlwind-tour-of-e-exprs">A Whirlwind Tour of E-Exprs</h2>
<p>The leaves of an eexpr are atoms such as numbers, symbols, and strings. A range of Unicode symbols<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> are also supported, and there is no distinction between different classes of symbols (such as operator, variable). In addition to C-style strings, there are also also be written sql-style, or as heredocs, which are equivalent in the abstract grammar. All other eexprs are simply various combinations of these leaves. FIXME move this to a more detailed place where I can include rationale: There is no special syntax for character literals.</p>
<pre><code># numbers
12
0xf00d
6.636e-34
# symbols
filename
if
&gt;&gt;=
λ
a+3Ω
# strings
&quot;Hello!&quot;
&#39;wouldn&#39;&#39;t&#39;&#39;ve&#39;
&quot;&quot;&quot;
long form
  text that
 is &quot;uninterpreted&quot;
&quot;&quot;&quot;</code></pre>
<p>The simplest ways to combine eexprs is to simply write them next to each other separated by spaces, or to enclose them in parenthesis. An eexpr can also be enclosed in square or curly braces, or separated by commas or semicolons. Each of these enclosers and separators are treated distinctly by eexprs, but an eexpr-based language may choose to ignore distinctions. There are no restrictions as to which separators can be used with which enclosers, or even that either needs the other at all.</p>
<pre><code>greet name
(a + b)
[1, 2, 3]
{a b}; c; d</code></pre>
<p>C-style strings can be made into templates that embed further eexprs in backticks within the string. Unlike some other string template syntaxes, arbitrary eexprs can be spliced into a template.</p>
<pre><code>&quot;Hello, `name`!&quot;
&quot;`results.len` result`if results.len == 1 then &quot;&quot; else &quot;s&quot;` found&quot;</code></pre>
<p>Eexprs can also be separated by dots, just as long as there is no whitespace around the dots. To support more familiar syntax, the dot is optional before strings and enclosers.</p>
<pre><code># all dots present
player.[1].position
r.&#39;\bwords?\b&#39;.search.(&quot;some words&quot;)
# equivalents:
player[1].position
r&#39;\bwords?\b&#39;.search(&quot;some words&quot;)</code></pre>
<p>Eexprs can also be combined in an indented block, which are indicated by an end-of-line colon. Most of the time, the indented block should be a space-separated child of the other eexprs on the starting line rather than a child of the last dot-separated eexpr. Thus, the space before an indented block is optional.</p>
<pre><code># no-nice-things version
do :
  thing-1
  thing-2
# equivalent, but more familiar syntax
do:
  thing-1
  thing-2
# explicit dot-expressions are still possible
do.:
  thing-1
  thing-2</code></pre>
<p>Since indented blocks are often enclosed, an end-of-line open encloser implicitly starts an indented block:</p>
<pre><code># this natural syntax
do {
  thing-1
  thing-2
}
# is equivalent to
do {:
  thing-1
  thing-2
}</code></pre>
<p>Two eexprs can also be separated by a colon. These separating colons bind more tightly than comma, which makes it easy to encode key-value dictionaries in eexprs. Commas are optional both before and after a comma-separated expression, and likewise for semicolons. With proper supoprt from the client language, we can use indented blocks to allow the omission of oft-forgotten end-of-line commas.</p>
<pre><code>{
  type: &quot;point&quot;
  altitude: 0
  , lat: 51.388, lon: 30.099
}</code></pre>
<p>Finally there are a few minor syntaxes, such as ellipsis and prefix dot which we will go over in detail later. And of course there are line comments introduced by a hash—which we have already seen.</p>
<p>In these examples, I have written eexprs suggestive of some underlying semantics so as to motivate the features. However, it is important to note that no specific semantics are imposed by eexprs. E.g. a client of eexprs could encode function calls with the Haskell-like <code>f x</code>, Lisp-like <code>(f x)</code>, Algol-like <code>f(x)</code>, Forth-like <code>x f</code>, or in innumerable other ways—assuming the client language supports function calls at all.</p>
<p>TODO: once I implement these features, document them here:</p>
<ul>
<li>mixfixes</li>
<li>ascii &lt;-&gt; unicode</li>
</ul>
<h2 id="abstract-syntax">Abstract Syntax</h2>
<p>E-exprs form a recursive algebraic data type, where each constructor represents a widely-recognized programming construct:</p>
<pre><code>data Eexpr
  = Symbol    ShortText
  | Number    Bignum
  | String    ShortText [(Eexpr, ShortText)]
  | Paren     (Maybe (Eexpr))
  | Bracket   (Maybe (Eexpr))
  | Brace     (Maybe (Eexpr))
  | Block     (NonEmpty (Eexpr))
  | Predot    (Eexpr)
  | Chain     (NonEmpty2 (Eexpr))
  | Space     (NonEmpty2 (Eexpr))
  | Ellipsis  (Maybe (Eexpr)) (Maybe (Eexpr))
  | Colon     (Eexpr) (Eexpr)
  | Comma     [Eexpr]
  | Semicolon [Eexpr]

data NonEmpty a = a :| [a]
data NonEmpty2 a = a :|| NonEmpty a</code></pre>
<p>A sequence of dot-separated expressions is represented with the <code>Chain</code> constructor. I though the name <code>Dots</code> was a little misleading because the dots are sometimes optional, as in <code>array[i]</code>, which is a <code>Chain</code> that contains no dots at all in the concrete syntax.</p>
<p>The one constructor that isn’t widely-recognized is <code>Predot</code>, which represents an expression prefixed by a single dot.</p>
<div class="Rationale">
<p>In Haskell, named functions can be made into binary infix operators—e.g. <code>x `member` s</code> rather than <code>member x s</code>. Originally I wanted to generalize this so that arbitrary expressions can be made into infix operators—e.g. <code>map .(insert k) v</code>. However, I quickly realized that this could also be used to create “artifical methods”—e.g. rewrite <code>get theMap .insert k (compute blarg)</code> to <code>insert (get theMap) k (compute blarg)</code>. Even more generally, a prefix dot could be used to determine an alternate meaning of the expression it attaches to. In pattern matching for example, one must determine if a symbol is a constructor or a named hole; a prefix dot could be used on holes to distinguish them, rather than (say) enforcing a case convention. Regardless, it is up to the client language to determine the meaning of a prefix dot in various contexts, if it allows prefix dot at all; eexprs simply make the syntax available if there <em>is</em> a need.</p>
</div>
<h2 id="recognizer-combinators">Recognizer Combinators</h2>
<p>FIXME: this paragraph is garbo. This grammar is representable by a simple ADT. To transform an eexpr term into a more specific syntax, you could simply pattern-match against this ADT. However, if you want to report (client) grammatical errors in context, then it it likely preferable to use an arrow-based api to match eexprs. I have implemented such such an api. It also includes an instance for <code>ArrowApp</code>, which technically means that it could be lifted into a monad; however, I am skeptical of the monadic style’s ability to correctly track pattern-matching context (without careful programmer diligence, which I know from personal experience to be a pipe dream).</p>
<h2 id="concrete-syntax">Concrete Syntax</h2>
<p>At least conceptually, the grammar of eexprs can be split into four phases:</p>
<ol type="1">
<li>Decode UTF-8</li>
<li>Raw Lexer</li>
<li>Token Cooker</li>
<li>Parser</li>
</ol>
<p>Raw lexing identifies individual tokens in a Unicode stream. It can be described mostly with regular expressions, though at points we require capturing groups. In the reference implementation, we require at most three codepoints of lookahead.</p>
<p>Cooking the token stream involves examining tokens in context to determine if they are relevant to the parser, and sometimes disambiguate how the token is to be used (e.g. space that separates eexprs vs. space which indicates indentation). This is described by a term rewrite system which only uses a few (TODO how many?) tokens of lookaround.</p>
<p>Finally, the parser consumes the cooked token stream to produce an eexpr according to a context-free grammar.</p>
<div class="Rationale">
<p>The Unicode character set seems the most future-proof choice, since its goal is to subsume all other character sets. Choosing UTF-8 as the only encoding is less flexible, but detection of magic strings in their appropriate encodings is a source of complexity. Would that complexity be worth it? I don’t think so, and the <a href="http://utf8everywhere.org/">UTF-8 Everywhere Manifesto</a> presents the argument better than I can here.</p>
</div>
<h3 id="parser">Parser</h3>
<p>Since the raw lexer handled all the tricky parts of decoding a character stream, and the lexeme cooker handled almost all the tricky work of making sure the tokens relate to each other intuitively, the parser is refreshingly straightforward.</p>
<pre><code>Expr ::= number
      |  symbol
      |  StringTemplate
      |  EnclosedExpr

StringTemplate
  ::= string(S, S)
   |  string(S, T) SpaceExpr (string(T, T) SpaceExpr)* string(T, S)

# FIXME I haven&#39;t actually done the rewriting for indent/newline/dedent tokens
EnclosedExpr
  ::= encloser(Open, a) SemicolonExpr? encloser(Close, a)
   |  indent(n) BlockLine (nl(n) BlockLine)* dedent(n)

TODO I might allow leading comma/semicolon for block lines
FIXME I think it&#39;d be work allowing `EllipsisExpr colon SemicolonExpr
FIXME also, `EllipsisExpr? semicolon SemicolonExpr` and likewise for comma exprs
BlockLine
  ::= EllipsisExpr semicolon SemicolonExpr
   |  EllipsisExpr comma CommaExpr (semicolon SemicolonExpr)?
   |  EllipsisExpr colon EllipsisExpr
   |  EllipsisExpr

SemicolonExpr ::= semicolon? CommaExpr (semicolon CommaExpr)* semicolon?
               |  CommaExpr

CommaExpr ::= comma? ColonExpr (comma ColonExpr)* comma?
           |  ColonExpr

ColonExpr ::= EllipsisExpr colon EllipsisExpr
           |  EllipsisExpr

EllipsisExpr ::= SpaceExpr? ellipsis SpaceExpr?
              |  SpaceExpr

SpaceExpr ::= ChainExpr (space ChainExpr)+
           |  ChainExpr

ChainExpr ::= Expr ChainTail+
           |  Expr
ChainTail ::= dot Expr
           |  EnclosedExpr
           |  StringTemplate</code></pre>
<h3 id="lexeme-cooker">Lexeme Cooker</h3>
<p>The lexeme cooker is a string rewrite system over tokens It deals with mostly local interactions between tokens, especially normalizing whitespace and whitespace-sensitive tokens.</p>
<p>The basic form of rewrite rule is <code>a ⇒ b</code> which replaces the token sequence <code>a</code> in the input with tokens on the right for output. We also allow the use of “non-terminal” patterns on the left of a rule; the patterns we use can be modeled as regular languages over tokens. We also use lookahead and lookbehind (collectively lookaround) in the left-hand side of a pattern. These affect only whether the rewrite rule fires, but the sequences matching the lookaround are not rewritten. We write <code>&lt;l&gt;</code> for lookaround which must match for a rule to fire, and <code>!&lt;l&gt;</code> for lookaround which must <em>not</em> match; of course, lookaround is only allowed at the start and/or end of the left-hand side of a rule.</p>
<p>The rewrite system may also be required to report an error, which is written <code>lhs ⇒ ERR</code>. There are a number of places where an implementation may warn the user, written <code>lhs ⇒ WARN rhs</code>, or as <code>lhs ⇒ WARN</code> when no rewriting takes place. With appropriate configuration, warnings may also be elevated to errors, but eexprs which generate warnings are nevertheless valid and should be accepted by default.</p>
<div class="Rationale">
<p>The characteristic that distinguishes some conditions as warnings rather than errors is that those conditions could be unambiguously machine-corrected in the source code without a change to the resulting eexpr.</p>
</div>
<p>TODO: I am less familiar with the properties of rewrite systems, but I think these could be applied each in-order in a single pass over the string, or make several passes one for each. TODO: I’m also not sure how much these rules could be re-ordered, but I do know some must come before others (e.g. whitespace classification before indentation). Rules listed earlier in this system take precedence over rules listed later.</p>
<p>Before cooking, the raw token stream is augmented with a start-of-file token <code>SOF</code> at the start and an <code>EOF</code> token at the end. Once cooking is complete, these special tokens are then removed. Implementations perhaps need not actually allocate space for such tokens; this practice just makes it easier to specify.</p>
<h4 id="oddball-rules">Oddball Rules</h4>
<p>Two commonly-used token patterns are for start- and end-of-line.</p>
<pre><code>sol ::= nl | SOF
eol ::= nl | EOF</code></pre>
<p>It is recommended that every file of eexprs end with a trailing newline.</p>
<pre><code>!sol EOF ⇒ WARN</code></pre>
<div class="Rationale">
<p>Given a set of valid files of eexprs that end with at least one trailing newline, any simple concat of these files would produce a valid file of eexprs. This property could come in handy when assembling eexprs mechanically. In particular, a compiler could simply prepend the dependencies of a module and compile the whole program at once, rather than needing to define an interface file format and perform linking. One of the goals of eexprs is to make it faster to produce reasonable quality experimental languages, so a feature that makes it easier to support multi-file source code is a no-brainer.</p>
</div>
<p>Mixed whitespace (adjacent tabs and spaces) is an error. It should be impossible for the lexer to produce adjacent whitespace tokens of the same type, but just in case, we can also merge such tokens at this point.</p>
<pre><code>lws(x, _) lws(y ≠ x, _) ⇒ ERR
lws(x, m) lws(x, n) ⇒ lws(x, n + m)`</code></pre>
<p>Some rules are cumbersome to specify in this rewrite system, but are simple enough that prose should suffice:</p>
<ul>
<li>If there are two types of indentation (start-of-line tabs-vs-spaces) in-use, that is an error (TODO iirc, this includes heredoc indentation, not just line-leading whitespace tokens).</li>
<li>If a single file uses two or more types of newline, that is a warning.</li>
<li>Whenever there is trailing whitespace token (not necessarily whitespace characters, which may be part of a token), that is a warning.</li>
</ul>
<div class="Rationale">
<p>In the endless religious war of tabs vs. spaces, there is only one real issue that bugs me: alignment. Namely, I think humans should not be aligning code by hand—it’s too fragile to justify the tedium. Somehow, we need to get machines to align code for us. Proposals have been made to enhance editors with smarter features, but I think it is unreasonable to expect all editors to choose the same methodology, or even to upgrade at all.</p>
<p>I propose a separate tool which can operate over any text file, searching for special characters that indicate where alignment into a table should take place. Unfortunately, there isn’t a clearly good choice for an alignment indicator character. Importantly, whatever indicator is chosen should remain in-place so that code can be re-indented without re-inserting the indicator; thus, we cannot simply use any sequences of tabs and spaces, even if we were to ban one of them from all other uses. If we could just pick a codepoint without care, I would probably go for a zero-width space, or possibly repurpose one of the disused ASCII control characters; these are unfortunately not easy to type on any standard keyboard. For accessibility, backslash-space might be the best, but I expect it to have poor aesthetics, and possibly have complex interactions with tabs characters.</p>
<p>In any case, it is reasonable to expect that some whitespace handling in eexprs will be adjusted in the future to enable the use of an external alignment tool.</p>
</div>
<h4 id="normalize-whitespace">Normalize Whitespace</h4>
<p>Comments and blank lines are ignored, as is intuitive. When joining lines, only the leading whitespace of the final line is retained.</p>
<pre><code>lws(_, _)? comment? &lt;eol&gt; ⇒ ɛ

lws(_, _) lineJoin ⇒ lineJoin
lineJoin lineJoin ⇒ lineJoin
lineJoin lws(_, n) ⇒ lws(_, n)

nl &lt;eol&gt; ⇒ ɛ</code></pre>
<p>Linear whitespace immediately inside enclosers is ignored. E.g. <code>(a)</code> and <code>( a )</code> are indistinguishable to the grammar. The same logic applied to string templates, with appropriate adjustments for the middle parts of the string.</p>
<pre><code>&lt;encloser(Open, _)&gt; lws(_, _) ⇒ ɛ
lws(_, _) &lt;encloser(Close, _)&gt; ⇒ ɛ

&lt;cStr(_, T)&gt; lws(_, _) ⇒ ɛ
lws(_, _) &lt;cStr(T, _)&gt; ⇒ ɛ</code></pre>
<h4 id="indentation">Indentation</h4>
<p>The logic for indentation is that an end-of-line colon indicates that the next leading whitespace defines an indent level. Each logical/abstract line of an indented block must begin with the same amount of leading whitespace that defines the level of the block. A logical line may span multiple physical lines, but only of it obeys the “offsides rule”: extra physical lines must begin with more leading whitespace than the level of indent. Dedents are inserted whenever a line begins with less than the current indent level. However, dedents must return to an active indentation level.</p>
<pre><code>block1:
  block2: # activate an indent at two spaces
    do lots of stuff # activate an indent at four spaces
    do lots    # | these two lines are equivalent
      of stuff # | to the line above
  end block2 # deactivate the four-space indent
 illegal # | this line starts with one space,
         # | which is not an active indentation level</code></pre>
<p>It is easy to insert indentation, since it is always starts at an end-of-line colon or open encloser. We also insert a synthetic space before indentation-colons, unless that colon is preceded by a dot. At the same time, we can classify other unknown colons. Separator colons must be followed by linear whitespace, but space around them is irrelevant to the grammar. Colons not followed by whitespace (linear or newline) are an error.</p>
<pre><code>!&lt;sol | unknownDot | lws(_, _)&gt; unknownColon nl lws(_, n) ⇒ space indent(n)
!&lt;sol | unknownDot | lws(_, _)&gt; unknownColon !&lt;lws(_, _)&gt; ⇒ space indent(0)
&lt;sol | unknownDot | lws(_, _)&gt; unknownColon nl lws(_, n) ⇒ indent(n)
&lt;sol | unknownDot | lws(_, _)&gt; unknownColon !&lt;lws(_, _)&gt; ⇒ indent(0)
&lt;encloser(Open, x)&gt; nl lws(_, n) ⇒ indent(n)
&lt;encloser(Open, x)&gt; ɛ !&lt;lws(_, _)&gt; ⇒ indent(0)

lws(_, _)? unknownColon lws(_, _) ⇒ colon
unknownColon !&lt;lws(_, _)&gt; ⇒ ERR # FIXME implement this in C</code></pre>
<div class="Rationale">
<p>I expect that indented blocks will much more often be the final element of a space-separated sequence of eexprs, rather than the final element of a dot-separated sequence. Thus, rather than group indent/dendent with other enclosers, if you wish to dot-separate an indented block, the dot is necessary. Inserting a synthetic space makes lines such as <code>do:</code> and <code>do :</code> equivalent, and thus allows the indentation colon to be placed directly against an eexpr rather than requiring an unnatural space to separate them. If a block at the end of a dot-separated sequence is desired, explicit dots <code>do.:</code> can achieve it.</p>
</div>
<div class="Rationale">
<p>When an open encloser is at the end of the line, the following lines are almost always indented, be it for statements in a block, elements in a list, arguments for a function, or what-have-you. Thus, eexprs automatically begin an indent immediately after a line ending with an open encloser. This may seem redundant, but is exactly this redundancy which helps detect inconsistencies between the structure of bracketing and the structure of layout, which have caused security vulnerabilities (at least one I know of, which means there must be others that I do not know of, and others still that were not made public). This behavior can be suppressed if desired with line joining.</p>
</div>
<div class="Rationale">
<p>I’m still debating whether to allow colons in symbols, and whether to allow fat colons. By requiring space after a colon, any decision can be made backwards-compatibly.</p>
<p>FIXME I’m thinking more about whether to join a colon-nospace-symbol into a single symbol, but there’s not a good way to express this in the rewrite rules. Perhaps it can go in the symbol raw lexer rule for symbols, but it doesn’t belong to this phase. I guess for now I’ll require lws after regular colons, just to play it safe. It’s not just ‘::’ that I want, but also ‘:=’ and especially ‘::=’. It seems I really do want to allow colons in identifiers in some capacity; I just have to make sure trailing colons are disallowed.</p>
</div>
<p>You may be able to see that the logic for inserting dedents is not presentable with the rewrite system we have so far. Instead, we will need to augment it with a stack. Now that we need it, we introduce the notation <code>a s⇒t b</code> for a rewrite rule which updates the initial state <code>s</code> to <code>t</code> while also rewriting <code>a</code> to <code>b</code>. When <code>s</code> is left out, that indicates that the initial state is irrelevant, and when <code>t</code> is left out, there is no change to the state. We use a non-empty cons list to as the stack, and use Haskell-like notation for it (<code>x:xs</code> for cons, and <code>[x]</code> for singletons).</p>
<p>These next rules use the state to ensure that indents are valid (they are deeper than any active indent) and insert dedents. The initial state for the rewrite system is <code>[0]</code>. It makes sure that dedents are valid by using two rules for when leading whitespace is greater than the active indentation level. If we are not already dedenting, we begin dedenting, which leaves a trail of dedent tokens behind. So, if we detect leading whitespace greater than the active indent level, but have just seen a dedent, we know that a dedent has failed to return to a known indent.</p>
<pre><code>indent(n &gt; lvl) (lvl:rest)⇒(n:lvl:rest) indent(n)
indent(n ≤ lvl) (lvl:rest)⇒ ERR

nl zlws(n &lt; lvl) (lvl:rest)⇒(rest) dedent(lvl) nl lws(n)
nl zlws(n = lvl) (lvl:rest)⇒ nl(n)
!&lt;dedent(_)&gt; nl zlws(n &gt; lvl) (lvl:rest)⇒ space
&lt;EOF&gt; (lvl:rest)⇒(rest) dedent(lvl)

&lt;dedent(_)&gt; nl zlws(n &gt; lvl) (lvl:rest)⇒ ERR</code></pre>
<div class="Rationale">
<p>I experimented with no requiring dedents to return to a known indentation level, which would be a simpler algorithm to describe. However, I think it is too difficult to see when there are spaces after a dedent, even though that makes a significant change to the structure of the eexpr. Client languages which wish to space-separate blocks should instead enclose those blocks. For example, the following is a valid eexpr with two space-separated (enclosed and) indented blocks</p>
<pre><code>if condition then {
  thing 1
  thing 2
} else {
  other thing
}</code></pre>
</div>
<p>Out final rule for indentation is that the first eexpr in a file cannot have leading whitespace. Then, all other eexprs in a file are (proper) subexpressions of the first, or else part of another eexpr on a different logical line.</p>
<pre><code>SOF lws(n) ⇒ ERR</code></pre>
<p>At this point, all trailing whitespace has been removed and all leading whitespace converted into indentation structure. Thus, we can transform all remaining linear whitespace into simple space tokens.</p>
<pre><code>lws(_, _) ⇒ space</code></pre>
<h4 id="space-sensitive-constructs">Space-Sensitive Constructs</h4>
<p>Finally, we look at tokens which are sensitive to surrounding whitespace. The first of these are dots, which must be classified into</p>
<ul>
<li>prefix dots, which have a space before them, but not after,</li>
<li>chain dots, which have no space before or after, and</li>
<li>all other combinations of dot and whitespace are illegal.</li>
</ul>
<pre><code>!&lt;space | sol&gt; unknownDot !&lt;space | eol&gt; ⇒ dot
&lt;space | sol&gt; unknownDot !&lt;space | eol&gt; ⇒ prefixDot
unknownDot &lt;space | eol&gt; ⇒ ERR</code></pre>
<p>Finally, we are left with token sequences which can be difficult for humans to read quickly. For example, <code>1+2</code> would parse as a sequence of two tokens <code>1</code> and <code>+2</code>, and <code>a+b</code> would parse as a single symbol. We rule these sequences out with the following rules:</p>
<pre><code># symbols and numbers have too much in common to smush together
(number | symbol) (number | symbol) ⇒ ERR

# don&#39;t make readers try to count dots in a row
(dot | ellipsis | prefixDot) (ellipsis | dot) ⇒ ERR

# a dot after a number could be confused for a decimal point
number dot ⇒ ERR

# adjacent strings looks too much like C implicit string catenation
cStr(_, S) cStr(S, _) ⇒ ERR</code></pre>
<h3 id="raw-lexer">Raw Lexer</h3>
<p>TODO what all metasyntax do I use? only introduce it just before it is needed, not all at once</p>
<pre><code>token ::= nl | lws(_, _) | lineJoin | comment
       | number | symbol
       | TODO</code></pre>
<h4 id="whitespace">Whitespace</h4>
<p>TODO these are the bits of whitespace that are used all over the place</p>
<pre><code>NL ::= &lt;U+000A New Line&gt;
CR ::= &lt;U+000D Carriage Return&gt;
RS ::= &lt;U+001E Information Separator Two&gt;
nl ::= NL CR? | CR NL? | RS
# additional types of newline may be considered
# it is also possible for implementations to ignore obsolete newline sequences; I&#39;ve included everything from wiki just because I could

lws(SP, n ≥ 1) ::= &lt;U+0020 Space)&gt;{n}
lws(HT, n ≥ 1) ::= &lt;U+0009 Character Tabulation&gt;{n}
# additional types of space may be considered, but each must get a distinct whitespace type</code></pre>
<p>TODO these are whitespace-like things</p>
<pre><code>BS ::= &lt;U+005C Reverse Solidus&gt;
lineJoin ::= BS lws(_, _)? nl

comment ::= &#39;#&#39; (!nl)*</code></pre>
<h4 id="numbers-and-symbols">Numbers and Symbols</h4>
<p>E-exprs support both integer and fractional numbers in a variety of radices (bases). Doing this requires a surprising number of rules, but the logic is fairly intuitive.</p>
<p>At their core, numbers consist of an integer part, an optional mantissa, and an optional exponent. For numbers not in base ten, a different radix can be indicated by beginning the integer part with a radix-leader (<code>0x</code> and the like). When there is a mantissa, there must be digits on both sides of the decimal point (e.g. <code>0.0</code> is allowed, but neither <code>0.</code> nor <code>.0</code> ). The exponent is essentially the same as the integer part, but only fractional numbers are allowed to have signed exponents. There are a variety of ways to indicate the start of an exponent, dependent on the base.</p>
<pre><code>number ::= sign? intPart(base) uexp(base)?
        |  sign? intPart(base) fracPart(base) exp(base)?

intPart(base) ::= radixLeader(base) digits(base)
fracPart(base) ::= &#39;.&#39; digits(base)

uexp(base) ::= expLetter(base) digits(base)
            |  &#39;^&#39; radixLeader(expBase) digits(expBase)
exp(base) ::= sign? uexp(base)</code></pre>
<div class="Rationale">
<p>Fractional numbers must have both integer and mantissa present, since there’s really no reason to omit either. I think the extra character on either side is a negligible cost; if you find it it significant, I would wonder why you have so many magic constants in your code. Plenty of style guides require the zero in all or most contexts, and including it makes space in the grammar to distinguish numbers from other syntaxes. While the prefix-dot syntax I discuss below would have conflicted with an optional zero integral part for numbers, I had already decided on the numerical syntax before developing the prefix-dot. Perhaps it was dumb luck that I didn’t have to work hard to squeeze an idea into the syntax, but I flatter myself to think that early preparation enabled that luck.</p>
</div>
<p>E-exprs support bases 2, 8, 10, 12, and 16. Unsurprisingly, base 10 is the default, and the digits are exactly as you might expect.. To allow visual grouping, digits may be separated by underscores, but underscores should not appear at the start of end of a string of digits.</p>
<pre><code>digits(base) ::= digit(base)+ ((&#39;_&#39; | digit(base))* digit(base))?

digit(2) ::= &#39;0&#39; | &#39;1&#39;
digit(8) ::= &#39;0&#39;…&#39;7&#39;
digit(10) ::= &#39;0&#39;…&#39;9&#39;
digit(12) ::= radixChar(10) | (&#39;↊&#39; | &#39;X&#39;) | (&#39;↋&#39; | &#39;E&#39;)
digit(16) ::= radixChar(10) | &#39;a&#39;…&#39;f&#39; | &#39;A&#39;…&#39;F&#39;</code></pre>
<div class="Rationale">
<p>Allowing underscores in numbers makes it possible to group digits in ways that make large numbers easy to identify by subitizing rather than the more costly method of counting. Allowing multiple underscores makes it possible to have multiple levels of grouping; as numbers get larger, it becomes difficult to idenify how many groups there are. Consider <code>18_446_744__073_709_551_616</code> vs. <code>18_446_744_073_467_709_551_616</code> vs. <code>18446744073709551616</code>: it is easy to see that the first is about 18 million billion, but in the second I have to count the groups to estimate 18 <em>billion</em> billion, and in the third I have to count every digit to verify that it’s back to 18 million billion. I picked this example specifically because I had trouble remembering roughly how much <span class="math inline">\(2^{64}\)</span> is: unlike <span class="math inline">\(2^{32}\)</span>, there are just too many thousands groups to subitize.</p>
</div>
<div class="Rationale">
<p>Base twelve is thrown in there basically just for fun, though it does help that the Unicode Consortium added dedicated codepoints for dec and el digits. The alternate ASCII glyphs <code>X</code> and <code>E</code> were chosen because the Dozenal Society of America voted for those forms where Unicode is poorly supported.</p>
<p>More seriously, I had considered bases 32, 62, and 64. However, the alphabets for these bases are not widely agreed-on, so I leave those notations to client grammars.</p>
<p>If it were easier to input Cuneiform, and I had a good way to solve the problem of representing zeros, I might have added <em>it</em> too—again for the lulz.</p>
</div>
<p>Radix indicators are the commonly-used ones. There is no indicator for ten (and I have marked this explicitly in the grammar), since base ten is the default. Exponents—you may have seen above—can always be marked with a <code>^</code> character, which also allows a change of radix.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> There are also some radix-specific exponent markers for radices where there is some agreement: namely in bases 2, 10, and 16.</p>
<pre><code>sign ::= &#39;+&#39; | &#39;-&#39;

radixLeader(base) ::= &#39;0&#39; radixLetter(base)
radixLeader(10) ::= ɛ

radixLetter(2) ::= &#39;b&#39; | &#39;B&#39;
radixLetter(8) ::= &#39;o&#39; | &#39;O&#39;
radixLetter(10) ::= Ø
radixLetter(12) ::= &#39;z&#39; | &#39;Z&#39;
radixLetter(16) ::= &#39;x&#39; | &#39;X&#39;

expLetter(2) ::= &#39;b&#39; | &#39;B&#39;
expLetter(8) ::= Ø
expLetter(10) ::= &#39;e&#39; | &#39;E&#39;
expLetter(12) ::= Ø
expLetter(16) ::= &#39;h&#39; | &#39;H&#39;</code></pre>
<p>Eexpr implementations should be careful when choosing an internal representation for parsed numbers. It is possible for a naïve implementation to consume all system memory. For example, numbers such as <span class="math inline">\(10^{(10^{100})}\)</span> can be represented in an eexpr with as few as 103 bytes, but when represented as an exact bigint, would require more memory than humans have ever manufactured. In the reference implementation, we do not attempt to expand the exponent, and instead leave it to client languages to determine whether (and how) to represent a number or emit an error. To help with this (TODO implement this), we have also included grammar combinators which only succeed for numbers that fit into various common representations.</p>
<h4 id="symbols">Symbols</h4>
<p>TODO TODO make sure to alter symbolChar later if I change the name here FIXME implement these choices</p>
<pre><code>symbol ::= symbolChar1 symbolChar* - sign digit(10) symbolChar*

iIl,1o0O.

symbolChar ::= alphaNum | asciiSymb
            |  greekAlpha | hebrewAlpha
            |  mathAlpha | blackboard
            |  mathSymb | arrows | extraDots
            | unsortedCharsTODO
symbolChar1 ::- symbolChar - SQ

alphaNum ::= &#39;a&#39;…&#39;z&#39; | &#39;A&#39;…&#39;Z&#39; | &#39;0&#39;…&#39;9&#39; | &#39;↊&#39; | &#39;↋&#39;
asciiSymb ::=
  | &#39;!&#39; | &#39;$&#39; | &#39;%&#39; | &#39;&amp;&#39; | SQ | &#39;*&#39; | &#39;+&#39; | &#39;-&#39; | &#39;/&#39;
  | &#39;&lt;&#39; | &#39;=&#39; | &#39;&gt;&#39; | &#39;?&#39; | &#39;@&#39;
  | &#39;^&#39; | &#39;_&#39;
  | &#39;|&#39; | &#39;~&#39;

greekAlpha ::=
  | &#39;Α&#39;…&#39;Ω&#39; - &lt;U+03A2&gt; | &#39;α&#39;…&#39;ω&#39;
  # variant forms
  | &#39;ϐ&#39; | &#39;ϴ&#39; | &#39;ϑ&#39; | &#39;ϒ&#39; | &#39;ϕ&#39; | &#39;ϖ&#39; | &#39;ϰ&#39; | &#39;ϱ&#39; | &#39;ϵ&#39;
  # archaic and numeral
  | &#39;Ͱ&#39;…&#39;ͳ&#39; | &#39;ϗ&#39;…&#39;ϡ&#39; | &#39;Ϻ&#39;…&#39;ϻ&#39;
  # modified forms
  | &#39;϶&#39; | &#39;ϼ&#39;
hebrewAlpha ::=
  # TODO oh yeah, some hebrew letters get used in math as well
  | &#39;ℵ&#39;…&#39;ℸ&#39;

mathAlpha ::= # alternate alphanum fonts for math (𝒜𝒶, 𝓐𝓪, 𝔄𝔞, 𝔸𝕒)
  # TODO I removed bold fraktur b/c it&#39;s not different enough from normal fraktur
    &#39;𝒜&#39;…&#39;𝔷&#39; - &lt;reserved points in this range&gt;
  | &#39;ℬ&#39; | &#39;ℰ&#39; | &#39;ℱ&#39; | &#39;ℋ&#39; | &#39;ℐ&#39; | &#39;ℒ&#39; | &#39;ℳ&#39; | &#39;ℛ&#39;
  | &#39;ℯ&#39; | &#39;ℊ&#39; | &#39;ℴ&#39;
  | &#39;ℭ&#39; | &#39;ℌ&#39; | &#39;ℑ&#39; | &#39;ℜ&#39; | &#39;ℨ&#39;
blackboard ::=
    &#39;𝔸&#39;…&#39;𝕫&#39; - &lt;reserved points in this range&gt; | &#39;ℂ&#39; | &#39;ℍ&#39; | &#39;ℕ&#39; | &#39;ℙ&#39;…&#39;ℝ&#39; | &#39;ℤ&#39;
  | &#39;𝟘&#39;…&#39;𝟡&#39;
  | &#39;ℼ&#39; | &#39;ℽ&#39; | &#39;ℾ&#39; | &#39;ℿ&#39; | &#39;⅀&#39; # greek

mathSymb ::= setSymb | logicSymb
         | arithSymb | calculusSymb
         | mathRels | mathOps | mathTurnstiles
         | mathShapes
         | &#39;′&#39; # prime
         | extraMathSymb

extraDots ::= &#39;∴&#39; | &#39;∵&#39; | &#39;∷&#39;

setSymb ::=
  ∅∞
  ∩∪⨿×
  ⋂⋃∐⨉ # n-ary
  ∈∉∋∌
  ⊂⊃⊄⊅⊆⊇⊈⊉⊊⊋
logicSymb ::=
  ⊤⊥
  ¬∀∃∄
  ∧∨⊻⊼⊽
  ⅋ # linear logic (also uses &amp;!?⊗⊕, which are elsewhere)
  ⋀⋁ # n-ary
arithSymb ::=
  ±∓⋅÷√
  ∏∑⨊ # n-ary
  ‰‱
calculusSymb ::=
  ∂∆∇∫
mathRels ::= equivRels | compareRels
equivRels ::=
  ≠≡≢≣≟⩵⩶≝≜⩴≔≕ # &quot;equal&quot;
  ≃≄≅≇≆ #iso
  ∼≁≈≉≋ # approximate
  ≍≭≎≏⪮ # bendy
  ∝
compareRels ::=
  ≤≥⪇⪈≮≯≰≱ # straight
  ≶≷≸≹⋚⋛ # posets
  ≲≳≴≵⋦⋧ # approximate
  ≺≻≼≽⊀⊁⋠⋡⋞⋟ # cusp
  ⊏⊐⊑⊒⋢⋣⋤⋥ # square
  ⊲⊳⊴⊵⋪⋫⋬⋭ # closed
  ≬≪≫⋘⋙
mathOps ::=
  ∘⨾ # composition
  ⧺⧻∔∸ # plus and minus variants
  ≀ # wreath product
  ⊕⊖⊗⊙⊘⦼⨸⊚⊛⊜⊝
  ⊞⊟⊠⊡⧄⧇⧆
  ⨹⨺⨻
  ⨀⨁⨂ # n-ary
  ⋈⋉⋊
  ∗⋄⋆
mathTurnstiles
  ⊢⊨⊩⊫⟝
mathShapes ::=
  □⬠⬡○
  ◊⬭⬯
  ◇☆⯎⯏
  ⋕⩩ # I&#39;ve seen the last used in &quot;SICP for GR&quot; or whatever it&#39;s called


extraMathSymb ::=
# set theory
  ⨃⨄⨅⨆ # n-ary
  ⊌⊍⊎⋒⋓⊓⊔⩀⩁⩂⩃⩄⩅⩐⩎⩏
  ⋐⋑⪽⪾⪿⫀⫁⫂⫃⫄⫅⫆⫇⫈⫉⫊⫋⫌
# logic
  ⋎⋏
  ⌐⨼⨽
  ⟑⟇⟎⟏⩑⩒⩓⩔⩕⩖⩗⩘⩙⩚⩛⩜⩝⩞⩟⩠⩢⩣
  ⨇⨈ # n-ary
# arithmetic
  ⨪⨫⨬ # dotted minuses
# calculus
  ∬∭∮∯∰∱∲∳⨋⨌⨍⨎⨏⨐⨑⨒⨓⨔⨕⨖⨗⨘⨙⨚⨛⨜
# relations
  ≖≗≘≙≚≛≞ # straight
  ⩪⩫∻∽∾⩯ # wiggles
  ≂≊≌⋍⩬⩭⩰⩳ # straight and wiggly
  ≐⩦⩧≑≒≓⩷⩸⩮ # dotted
  ⋖⋗⋜⋝⩹⩺⩻⩼⪅⪆⪉⪊⪋⪌⪍⪎⪏⪐⪑⪒⪙⪚⪝⪞⪟⪠≦≧≨≩
  ⩽⩾⩿⪀⪁⪂⪃⪄⪓⪔⪕⪖⪗⪘⪛⪜⫹⫺
  ≾≿⋨⋩⪯⪰⪱⪲⪳⪴⪵⪶⪷⪸⪹⪺⪻⪼
  ∹⊰⊱
  ⪡⪢⫷⫸⪣⪤⪥⪪⪫⪬⪭⩤⩥⪦⪧⪨⪩
# operators
  ⩱⩲
  ⦶⦷⦸⦹⦺⦻⦽⧀⧁⧂⧃⨭⨮⨴⨵
  ⧅⧈⧉
  ⧊⧋⧌⧍⧎⧏⧐
  ⋋⋌
  ⋇⟐
  ⊹⊺ # I honestly have no idea what these are
# tacks
  ⊭⫤
  ⊬⊣
  ⊮⫣
  ⊯⫥
  ⫢⊪⫦
  ⟞
  ⫝̸⫝
  ⟘⟙⫧⫨⫩⫪⫫
# database
  ⟕⟖⟗
  ⨝⨞
# misc
  ∎ # QED/tombstone
  ∿
  ⦰⦱⦲⦳⦴⦵


# you can have one alternate head or tail, but not both
# you can also drop the head with an alternate tail
# there are no straight strokes, only slash strokes
arrows ::= #TODO fix metasyntax
  ←↑→↓↔↕ # straight
  ⇐⇑⇒⇓⇔⇕ # double
  ↚↛↮ # stroked
  ⇍⇎⇏ # stroked double
  ↜↝↭ # wave
  ⇜⇝ # squiggle
  ⤸⤹⤺⤻ # arc
  ⇚⇛⤊⤋ # triple
  ⟰⟱⭅⭆ # quadruple
# long
  ⟵⟶⟷ # long
  ⟸⟹⟺ # long double
  ⟻⟼ # long from bar
  ⟽⟾ # long double from bar
  ⟿⬳ # long squiggle
#alt heads
  ↞↟↠↡ # two headed
  ↼↽↾↿⇀⇁⇂⇃⥊⥋⥌⥍⥎⥏⥐⥑ # harpoon
  ⇤⇥⤒⤓ # to bar
  ⊸⟜⫯⫰⧟
# alt tails
  ↢↣ # tail
  ↩↪ # hook
  ↤↥↦↧ # from bar
  ↫↬ # loop
# no head, alt tail
  ⤙⤚ # only tail
  ⤛⤜ # only double tail
  ⥼⥽⥾⥿ # fish tail
# to/from endpoints
  ⤅⬶ # two headed from bar
  ⤆⤇ # double from bar
# turning
  ↰↱↲↳⬎⬏⬐⬑ # tip turn
  ⤴⤵⤶⤷ # curving
  ↶↷⤾⤿ # semicircle
  ⮌⮍⮎⮏ # (triangle-headed) u-shaped
  ↺↻ # circle
  ⟲⟳ # gapped circle
  ⥀⥁ # closed circle
# multiple
  ⇄⇅⇆⇇⇈⇉⇊⇵ # paired
  ⇋⇌ # paired harpoon
  ⇶⬱ # three
  ⥢⥣⥤⥥⥦⥧⥨⥩⥪⥫⥬⥭⥮⥯ # paired harpoons


unsortedCharsTODO ::=
  # TODO random stuff from letterlike symbols
  | &#39;ℓ&#39; | &#39;℘&#39; | &#39;№&#39; | &#39;℧&#39;
  | &#39;†&#39; | &#39;‡&#39; # or use these instead of emoji for unsafe functions</code></pre>
<div class="Warning">
<p>If you use non-ASCII notation in your language or library, it is not enough to include suggestions for ASCII alternatives. Instead, there must be a tool to convert losslessly between Unicode and ASCII.</p>
</div>
<div class="Warning">
<p>The selection of available symbol characters is not yet set in stone. So far, I have only examined the Basic Latin Unicode block for useful characters. I would like to allow mathematical operators, arrows, greek letters at least, and I may include a number of other scripts if it comes to it. However, I’ll also want to use some normalization algorithm to ensure visually-identical symbols are treated identically. I am loathe to include characters from scripts I am not fluent with; if they are to be added, it should be in consultation with people who <em>do</em> understand these scripts.</p>
<p>Here is a list of blocks that I have examined or plan to examine:</p>
<ul class="task-list">
<li><input type="checkbox" disabled="" checked="" />
Basic Latin</li>
<li><input type="checkbox" disabled="" checked="" />
math-related:<ul class="task-list">
<li><input type="checkbox" disabled="" checked="" />
Mathematical Operators</li>
<li><input type="checkbox" disabled="" checked="" />
Mathematical Alphanumeric Symbols</li>
<li><input type="checkbox" disabled="" checked="" />
Letterlike Symbols</li>
<li><input type="checkbox" disabled="" checked="" />
Miscellaneous Symbols</li>
<li><input type="checkbox" disabled="" checked="" />
Miscellaneous Symbols and Arrows</li>
<li><input type="checkbox" disabled="" checked="" />
Miscellaneous Mathematical Symbols-A</li>
<li><input type="checkbox" disabled="" checked="" />
Miscellaneous Mathematical Symbols-B</li>
<li><input type="checkbox" disabled="" checked="" />
Supplemental Mathematical Operators</li>
<li><input type="checkbox" disabled="" checked="" />
Latin-1 Supplement</li>
<li><input type="checkbox" disabled="" checked="" />
Geometric Shapes (I’ve only taken the white ones b/c they’re easy to print, and all of the same “size”)</li>
<li><input type="checkbox" disabled="" checked="" />
Miscellaneous Technical</li>
<li><input type="checkbox" disabled="" checked="" />
General Punctuation</li>
</ul></li>
<li><input type="checkbox" disabled="" checked="" />
arrows:<ul class="task-list">
<li><input type="checkbox" disabled="" checked="" />
Arrows</li>
<li><input type="checkbox" disabled="" checked="" />
Supplemental Arrows-A</li>
<li><input type="checkbox" disabled="" checked="" />
Supplemental Arrows-B</li>
<li><input type="checkbox" disabled="" checked="" />
Supplemental Arrows-C (no, these are mostly thickness variants and some wingdings-looking stuff, though there are a few arrowheads)</li>
<li><input type="checkbox" disabled="" checked="" />
Miscellaneous Symbols and Arrows</li>
</ul></li>
<li><input type="checkbox" disabled="" />
SI unit symbols (not a block)</li>
<li><input type="checkbox" disabled="" checked="" />
greek:<ul class="task-list">
<li><input type="checkbox" disabled="" checked="" />
Greek and Coptic</li>
<li><input type="checkbox" disabled="" checked="" />
Greek Extended</li>
</ul></li>
<li><input type="checkbox" disabled="" />
Hebrew</li>
<li><input type="checkbox" disabled="" />
keyboard key symbols (not a block)</li>
<li><input type="checkbox" disabled="" />
Latin-1 Supplement, Latin Extended-A -B -C -D and -E, Latin Extended Additional</li>
<li><input type="checkbox" disabled="" />
Combining Diacritical Marks, Combining Diacritical Marks Extended, Combining Diacritical Marks Supplement</li>
<li><input type="checkbox" disabled="" />
Combining Diacritical Marks for Symbols</li>
<li><input type="checkbox" disabled="" />
Combining Half Marks</li>
<li><input type="checkbox" disabled="" />
General Punctuation, Supplemental Punctuation</li>
<li><input type="checkbox" disabled="" />
Superscripts and Subscripts</li>
<li><input type="checkbox" disabled="" />
Currency Symbols</li>
<li><input type="checkbox" disabled="" />
Geometric Shapes, Geometric Shapes Extended, Miscellaneous Symbols, Dingbats, Ornamental Dingbats</li>
<li><input type="checkbox" disabled="" checked="" />
IPA extensions, Spacing Modifier Letters (I don’t think the IPA script is really useful for variable names)</li>
<li><input type="checkbox" disabled="" checked="" />
Number Forms (I already have the dozenal digits, the fractions are too limited in scope, roman numerals are unneeded)</li>
<li><input type="checkbox" disabled="" checked="" />
Control Pictures (I find it unlikely that adding these won’t introduce confusion)</li>
</ul>
</div>
<h4 id="strings">Strings</h4>
<p>Eexprs support three string formats to support different levels of escaping within the string. They are:</p>
<ul>
<li>C-style (or double-quoted strings),</li>
<li>SQL-style (or single-quoted strings), and</li>
<li>heredocs (or triple-quoted strings<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>).</li>
</ul>
<pre><code>string(x, y) ::= cString(x, y)
string(S, S) ::= sqlString
              |  heredoc</code></pre>
<p>C-style strings have the richest syntax for escapes, and are the only string style that can be used as a template. Instead of attempting to detect nesting level in the lexer, we instead detect fragments of templates and leave it to the parser to attempt assembly into sensible string template expressions. We therefore categorize C-style strings by how they might join with other string parts.</p>
<pre><code>SQ ::= &lt;U+0027 APOSTROPHE&gt;
DQ ::= &lt;U+0022 QUOTATION MARK&gt;
BQ ::= &lt;U+0060 GRAVE ACCENT&gt;

cStrDelim(S) ::= DQ
cStrDelim(T) ::= BQ
cStr(x, y) ::= cStrDelim(x) cStrUnit* cStrDelim(y)

cStrUnit ::= cStrChar
          |  BS cEscapeChar
          |  BS cStrLineJoin BS

HEX ::= &#39;0&#39;…&#39;9&#39; | &#39;a&#39;…&#39;f&#39; | &#39;A&#39;…&#39;F&#39;
cEscapeChar ::= BS | SQ | DQ | BQ
             |  &#39;n&#39; | &#39;r&#39; | &#39;t&#39;
             |  &#39;0&#39; | &#39;e&#39; | &#39;a&#39; | &#39;b&#39; | &#39;f&#39; | &#39;v&#39;
             |  &#39;x&#39; HEX{2} | &#39;u&#39; HEX{4} | &#39;U&#39; HEX{6}
             |  &#39;&amp;&#39;
cStrLineJoin ::= nl lws(_)

# TODO I should likely eliminate non-printing characters as well
cStrChar ::= 1 - (BS | DQ | BQ | nl)</code></pre>
<p>While most of the escape sequences are taken from C, a few are less-widely represented. The sequence <code>\e</code> encodes <code>&lt;U+001B ESCAPE&gt;</code>, and the sequence <code>\&amp;</code> encodes a zero-length string. The <code>\x</code>, <code>\u</code>, and <code>\U</code> sequences encode arbitrary codepoints from (respectively) ASCII, the Unicode Basic Multilingual Plane, and all of Unicode. Attempting to encode a codepoint outside the range of Unicode (anything larger than 0x10FFFF) is an error.</p>
<div class="Rationale">
<p>To be honest, I’m not sure that I need both <code>\&amp;</code> and the fixed-length <code>\x</code>/<code>\u</code>/<code>\U</code> escapes. The former was stolen from Haskell which allows variable-length Unicode escapes (e.g. <code>"\x333" !=</code>“33”<code>== \x33\&amp;3</code>). The latter is my own solution (though apparently C has something similar) to deal with the potential ambiguity of variable-length Unicode escapes by eliminating variable length entirely. It is entirely possible that I may change this area of the syntax before a 1.0 release in response to feedback.</p>
</div>
<div class="Rationale">
<p>While <code>\e</code> is not an escape compliant with the C Standard, this was because some character sets (e.g. EBCDIC) lack a character with the corresponding ASCII semantics. In the case of eexprs—which are explicitly reliant on Unicode—this is not a concern. Meanwhile, <code>\e</code> is used sometimes for terminal programming, and would likely be more meaningful for custom binary formats than the obscute “vertical tab” <code>\v</code> sequence.</p>
</div>
<div class="Rationale">
<p>Two C escapes <code>\?</code> and octal <code>\nnn</code> are missing from eexprs. The <code>\?</code> escape is to avoid trigraph syntax, a rarely-used C feature which eexprs do not include. Octal escapes on the other hand, are perfectly reasonable, but it seems few programmers use octal today. Even DEC, which famously used octal in all their PDP-11 and VAX documentation, switched to hexadecimal notation when they published the Alpha architecture.</p>
</div>
<p>SQL-style strings offer fewer escapes, but a compensatorily wider range of valid characters. SQL-style strings are delimited by single quotes, but a sequence of two single-quotes encodes a single quote into the string.</p>
<pre><code>sqlStr ::= SQ sqlStrUnit* SQ
sqlStrUnit ::= 1 - SQ
            |  SQ SQ</code></pre>
<div class="Rationale">
<p>I had considered using single-quotes to indicate character literals. However, I was also envious of syntax such as Python’s r-strings for easily writing strings which have lots of backslashes, such as regex. Instead of hardcoding a fixed (and therefore small) set of string prefixes, I instead opted to allow dot-separated symbol+string to omit the dot. Client languages can then determine any special interpretations for such a string, perhaps interpreting a string as a character. This then made space for SQL-style strings, which like Python r-strings can encode C-style escapes with ease.</p>
<p>In typed languages, Haskell’s <code>IsString</code> should serve as an inspiration: interpret a string literal as a character literal in contexts where a character-typed value is expected. The same technique can also be used for alternate memory layouts of strings, hexadecimal strings, base64 strings, ip addresses, mac addresses, and so on for as many types as users wish they had literals for.</p>
<p>In untyped languages, conversion functions (in general) cannot be inferred, and so must be manually inserted. This could be done by prefixing a string with a conversion function (perhaps in a separate namespace). Thus, <code>c'a'</code> for a character, <code>base64url"KMK044O7z4njg7sp44Gj55Sx"</code> for a base64-encoded bytestring, and so on.</p>
</div>
<p>Heredocs are multi-line strings that have no escaping whatsoever because their content is included verbatim. They begin with a line ending in a “triple quote” and optional identifier and end with a line beginning with that same identifier (if any) and another triple quote. All the intervening lines are included unaltered, including alternate newlines, control characters, and so on. Additionally, heredocs may also be indented by including a backslash after the opening identifier and another backslash to indicate the indent amount.</p>
<p>Indicating indent depth with a backslash on the first line of a heredoc allows for heredocs to start with leading whitespace. Accommodations have been made for aligning indented backslash despite the first-line backslash: when indentation is done with spaces, the backslash counts towards the number of spaces of leading indent, and when it is done with tabs, a tab must follow the backslash<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. The final newline before the close quotes is not included in the heredoc, so if you would like a heredoc to end with a trailing newline, leave a blank line.</p>
<pre><code>heredoc ::= startHeredoc(name, ws, n)
            heredocLine(name, ws, n)*
            endHeredoc(name, ws, n)

openHeredoc(\name) ::= DQ DQ DQ (?name = symbolChar*) lws(_, _)?
startHeredoc(name, _, 0) ::= openHeredoc(name) nl
startHeredoc(name, SP, n + 1) ::= openHeredoc(name) BS lws(_, _)? nl
                                  zlws(SP, n) BS
startHeredoc(name, HT, n + 1) ::= openHeredoc(name) BS lws(_, _)? nl
                                  zlws(HT, n) BS HT

heredocLine(name, ws, n) ::= zlws(ws, n) heredocContent nl
heredocContent ::= (1 - nl)* - (closeHeredoc(name) (1 - nl)*)

endHeredoc(name, ws, 0) ::= closeHeredoc(name)
endHeredoc(name, ws, n) ::= lws(ws, n) closeHeredoc(name)
closeHeredoc(name) ::= name DQ DQ DQ

zlws(_, 0) ::= ɛ
zlws(ws, n) ::= lws(ws, n)</code></pre>
<p>This makes them an attractive target for embedding non-eexpr content.</p>
<div class="Rationale">
<p>Heredocs are an attractive target for embedding non-eexpr content. I especially want to use if for embedding documentation as doc strings rather than comments<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. Writing documentation in comments makes it more difficult to attach them to language constructs. The difference can be seen very well in Python, which uses docstrings, and can therefore report documentation directly in the REPL, or during script execution. True, documentation could be written in an e-expr-based markup, but I’d likely prefer markdown—I’m not a zealot. Regardless, heredocs can embed any desired documentation language.</p>
<p>Another use for heredocs is embedding non-eexpr languages more generally. An obvious use would be for crafting SQL expressions directly in a web server. Other possibilities are embedding a scripting language such as Lua, or C code as part of a foreign function interface.</p>
</div>
<h4 id="punctuation">Punctuation</h4>
<p>TODO</p>
<pre><code>punctuation ::= encloser(_, _)
             | separator(_)
             | ellipsis
             | unknownDot
             | unknownColon

encloser(Open, Paren) ::= &#39;(&#39;
encloser(Open, Bracket) ::= &#39;[&#39;
encloser(Open, Brace) ::= &#39;{&#39;
encloser(Close, Paren) ::= &#39;)&#39;
encloser(Close, Bracket) ::= &#39;]&#39;
encloser(Close, Brace) ::= &#39;}&#39;

  # TODO more enclosers
  # ⌈⌉⌊⌋⟅⟆⟦⟧⟨⟩⟪⟫⟬⟭⟮⟯«»
  # ⦃⦄⦅⦆⦇⦈⦉⦊⦋⦌⦍⦎⦏⦐⦑⦒⦓⦔⦕⦖⦗⦘
  # ⧼⧽
  # how to rewrite in ASCII? perhaps use fat colon + parens `⟬f x y⟭ → ap::(f x y) ≡ f &lt;$&gt; x &lt;*&gt; y`

semicolon ::= &#39;;&#39;
comma ::= &#39;,&#39;

unknownDot ::= &#39;.&#39;
unknownColon ::= &#39;:&#39;
ellipsis ::= &#39;..&#39; # TODO probly also include &#39;…&#39;</code></pre>
<h2 id="suboptimality-of-s-exprs">Suboptimality of S-Exprs</h2>
<p>TODO now that this is in a different part of the report, it needs new linking verbiage</p>
<p>As powerful as the recognizer technique is, I don’t think s-exprs is its best medium. I don’t want to rant about s-exprs too much, since that’s not the point; if you stopped reading right now and used recognizers over s-exprs instead of writing parsers, your life would improve plenty. That said, if you have used enough s-exprs, you have probably experienced some pain points that e-exprs would improve on. At the other end of the spectrum, if you have resisted learning a Lisp because it looks too alien, then perhaps it would be better to jump directly to e-exprs.</p>
<p>The most important practical problem with s-exprs is a historical one: there is no standard for them. Different Lisp implementations have different notions of what constitutes an atom, and many have different concrete syntaxes for atoms even when they agree on the abstract syntax. Additionally, most implementations come with reader macros, which require a programming language to be well-defined before parsing into s-exprs can even be fully defined. Further, the “pure” s-exprs presented above have been “polluted” with a variety of syntactic shortcuts, such as the cons-dot <code>(a . b)</code>, a variety of quotation syntaxes, and syntax for keyword arguments; these extensions are another source of differences between implementations. The suspicious thing about these syntax extensions is that they are something no Lisp programmer would want to live without, but would contribute very little to non-Lisps.</p>
<p>The most common objection I’ve heard is that Lisps have “too many parenthesis”. While this appears to be a surface complaint, it would be foolish to ignore such persistent “customer” feedback. A more cogent version of this complaint would be that the meaning of parenthesis in Lisp are grossly overloaded. When faced with new Lisp code, one must figure out what each set of parenthesis is used for: do they invoke macros? procedures? project a struct member? delimit a list? a mapping? the items in a list or mapping? code blocks? statements within a block? and so on… Given more syntax, a language designer has more opportunity to visually distinguish common programming constructs.</p>
<dl>
<dt>E-expr</dt>
<dd><pre><code>match os.name:
  &quot;linux&quot;: display &quot;Hello, `fromMaybe name &quot;Linus&quot;`!&quot;
  &quot;mac&quot;: display &quot;Hello, `fromMaybe name &quot;Steve&quot;`!&quot;
  &quot;windows&quot;: {
    display &quot;Hello, `fromMaybe name &quot;Bill&quot;`!&quot;
    init-wsl
  }</code></pre>
</dd>
<dt>S-expr</dt>
<dd><pre><code>(match (os-name os)
  (&quot;linux&quot; (display (string-concat &quot;Hello, &quot; (fromMaybe name &quot;Linus&quot;) &quot;!&quot;))
  (&quot;mac&quot; (display (string-concat &quot;Hello, &quot; (fromMaybe name &quot;Steve&quot;) &quot;&quot;))
  (&quot;windows&quot; (begin
    (display (string-concat &quot;Hello, &quot; (fromMaybe name &quot;Bill&quot;) &quot;!&quot;)
    (init-wsl))))</code></pre>
</dd>
</dl>
<p>Of course, there are tools to help manipulate s-exprs, so one might argue that the ergonomics are not a real issue. However, tooling does not exist everywhere: unconfigured (or worse: badly-configured) editors, whiteboards and paper, web pages. A design philosophy I used for eexprs is “the easier it is to write with tooling, the easier it is everywhere”<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. Indeed, while I’m developing eexprs, I have no tooling for them, but I find it no more difficult than using sexprs with tools.</p>
<p>Before moving on, I just want to point out one non-advantage of s-exprs. I have heard it argued that the minimalist syntax of Lisp is the price you pay for Lisp’s most iconic advantage: its macro system. Indeed, one can very easily construct new sexprs from templates simply by introducing special forms for <code>quasiquote</code>, <code>unquote</code> and <code>unquote-splicing</code>. However, it is not necessary for s-exprs to be so minimalistic; it is sufficient that they form an algebraic data type. With an appropriate set of quasiquoters and unquoters, any language representable in an ADT can also take advantage of quasiquotation to implement metaprogramming. Nevertheless, I can see why in the 1960’s they went with sexprs rather than a richer system.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<h1 id="notes">NOTES</h1>
<p>parse a “level 0/1” int by checking that <code>radix^exp</code> is less than <code>2^n</code> for some reasonable <code>n</code> (perhaps 64 for known small ints, but if you allow exact big ints, then some larger n like <code>8^m</code> for a max of <code>m</code> bytes of memory (maybe 4096 is good? just how big a bigint do you need‽</p>
<p>If a colon has a newline on the right, it starts a block. If a colon has inline space on the right, it’s a normal colon. If a colon has no space on the right, it might be part of a symbol: it joins up with any symbols to the right or left it also joins up with any colon to the right recursively That way, I can have <code>Foo:bar.baz</code> be the <code>baz</code> record of the <code>Foo:bar</code> qualified name, just as long as I also have a way to split strings on colons.</p>
<p>Because eexprs draw a line between two algorithms both commonly referred to as “parsing” a “grammar”, we’ll use</p>
<ul>
<li>“eexpr grammar” and “parsing” for the grammar of plain eexprs and the parser that grammar, respectively</li>
<li>“client grammar” and “recognizing” for the grammar of a eexpr-based language and the parser/pattern matcher for that grammar, respectively</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>specifically: since 1958 with LISP<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>How useful unicode input turns out to be is a different question. I’ve set my system up to be able to input many mathematical characters, but it’s a lot of work to ask from users in general. Instead, I would like to offer tools to can translate eexprs between Unicode and ASCII representations; it would be up to language and library implementors to offer an ASCII alternative for each Unicode symbol they use.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Why you would want to change bases between base and exponent I don’t know…<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>even three double-quotes would be six quotes…?<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>So don’t set your tab size less than two. That’s what we call “expert guidance”!<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>In fact, I’ve only recently gotten over pronouncing “heredoc” as “docstring”.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>In a talk about accessibility, I picked up the idea that if you make something accessible for disabled people, you’ve probably made it easier for abled people as well. For example, a website that is usable by a person with dementia is not likely to be frustrating for the average user. This philosophy applies to the design of programming languages, and in a cybernetic sense, a lack of technology is a lack of ability.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>Have you ever tried to implement a compiler in 4096 <em>bytes</em> of memory?<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
